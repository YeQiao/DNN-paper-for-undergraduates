# Important-DNN-paper-for-undergraduates

1) Alexnet
   https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
   
2) ResNet
   https://arxiv.org/pdf/1512.03385.pdf

3) VGG
   https://arxiv.org/pdf/1409.1556.pdf

5) Faster R-CNN
   https://arxiv.org/pdf/1506.01497.pdf

6) SSD
   https://arxiv.org/pdf/1512.02325.pdf

7) COCO
   https://arxiv.org/pdf/1405.0312.pdf

8) ImageNet
   https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5206848

9) Attention Is All You Need
   https://arxiv.org/pdf/1706.03762.pdf

10) Swing Transformer
   https://arxiv.org/abs/2103.14030

11) Vision Transformer
   https://arxiv.org/pdf/2010.11929.pdf

12) Stable Diffusion
   https://arxiv.org/pdf/2112.10752.pdf


# Hardware-and-efficient-DNN-Papers

1)- Why Systolic Architecture ? 

http://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf

2)- Anatomy of High Performance Matrix Multiccation 

https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf

3)- Efficient Processing of Deep Neural Networks: A Tutorial and Survey

https://www.rle.mit.edu/eems/wp-content/uploads/2017/11/2017_pieee_dnn.pdf

4)- In-Datacenter Performance Analysis of a Tensor Processing Unit

https://arxiv.org/pdf/1704.04760.pdf

5)- SqueezeNext: Hardware-Aware Neural Network Design

https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf

6)- A Systematic Approach toBlocking Convolutional Neural Networks

https://arxiv.org/pdf/1606.04209.pdf

7)- High Performance Zero-Memory Overhead Direct Convolutions

https://arxiv.org/pdf/1809.10170.pdf

8)- MIXED PRECISION TRAINING

https://arxiv.org/pdf/1710.03740.pdf

9)- Mixed Precision Training With 8-bit Floating Point

https://arxiv.org/pdf/1905.12334.pdf

10)- A taxonomy of acceleratorarchitectures and their programming models

https://pdfs.semanticscholar.org/dee4/9d30a19f392ca9d002720a554800fa16d19e.pdf

11)- cuDNN: Efficient Primitives for Deep Learning

https://arxiv.org/pdf/1410.0759.pdf

12)- Eyeriss: A Spatial Architecture for Energy-Efficient Dataflowfor Convolutional Neural Networks

https://www.rle.mit.edu/eems/wp-content/uploads/2016/04/eyeriss_isca_2016.pdf

13)- Deep Learning Inference in Facebook Data Centers: Characterization,Performance Optimizations and Hardware Implications

https://arxiv.org/pdf/1811.09886.pdf

14)- In-Memory Computing: Advances and Prospects

https://ieeexplore.ieee.org/document/8811809

15)- Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding

https://arxiv.org/pdf/1510.00149.pdf

16)- Binarized Neural Networks

https://papers.nips.cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf

17)- XNOR-Net: ImageNet Classification Using BinaryConvolutional Neural Networks

https://arxiv.org/pdf/1603.05279.pdf

18)- NetAdapt: Platform-Aware Neural NetworkAdaptation for Mobile Applications

https://openaccess.thecvf.com/content_ECCV_2018/papers/Tien-Ju_Yang_NetAdapt_Platform-Aware_Neural_ECCV_2018_paper.pdf

19)- Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks

https://dl.acm.org/doi/10.1145/2684746.2689060

20)- Memory Requirements for Convolutional Neural Network Hardware Accelerators

https://ieeexplore.ieee.org/abstract/document/8573527

21)-DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning

https://dl.acm.org/doi/10.1145/2654822.2541967

22)- TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory

https://dl.acm.org/doi/10.1145/3093337.3037702

23)- Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks

http://blaauw.engin.umich.edu/wp-content/uploads/sites/342/2018/10/Eckert-Naural-Cache.pdf

24)- Cambricon-X: An accelerator for sparse neural networks

https://ieeexplore.ieee.org/document/7783723

25)- UCNN: Exploiting Computational Reuse in DeepNeural Networks via Weight Repetition

https://arxiv.org/pdf/1804.06508.pdf

26)- Hardware for Machine Learning:Challenges and Opportunities

https://arxiv.org/pdf/1612.07625.pdf

27)- Bit Fusion: Bit-Level Dynamically Composable rchitecture for Accelerating Deep Neural Networks

https://arxiv.org/pdf/1712.01507.pdf

28)- SCNN: An Accelerator for Compressed-sparseConvolutional Neural Networks

https://people.csail.mit.edu/anurag_m/papers/2017.scnn.isca.pdf

29)- EIE: Efficient Inference Engine on Compressed Deep Neural Network

https://www.cs.virginia.edu/~smk9u/CS6501F16/p243-han.pdf

30)- Understanding Sources of Inefficiency  in General-Purpose Chips

http://csl.stanford.edu/~christos/publications/2010.efficiency.isca.pdf
